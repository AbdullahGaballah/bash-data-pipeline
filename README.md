# bash-data-pipeline

A lightweight and fully logged Bash-based data pipeline that:

- Downloads a public CSV dataset  
- Cleans the data by removing empty or malformed rows  
- Performs basic analysis (record count, average height, and average weight)  
- Converts the cleaned CSV data into structured JSON format  
- Logs all steps with timestamps for traceability  

---

## ğŸ“ Project Structure

bash-data-pipeline/
â”œâ”€â”€ input/ # Raw downloaded CSV files
â”œâ”€â”€ cleaned/ # Cleaned CSV output
â”œâ”€â”€ reports/ # Analysis summary and JSON output
â”œâ”€â”€ logs/ # Log files for each pipeline run
â”œâ”€â”€ run_pipeline.sh # Main pipeline script
â””â”€â”€ README.md # Project documentation

 

## ğŸš€ How to Run

Ensure the script is executable and run it from the project root:

 
chmod +x run_pipeline.sh
./run_pipeline.sh

## ğŸ“¦ Output Files
input/data.csv â€“ raw downloaded dataset

cleaned/cleaned_data.csv â€“ cleaned version of the dataset

reports/summary.txt â€“ statistical summary (record count, average height, weight)

reports/data.json â€“ cleaned data in JSON format

logs/run.log â€“ detailed execution log with timestamps

## ğŸ“Š Dataset Source
Florida State University - hw_200.csv

## ğŸ› ï¸ Technologies Used
GNU Bash

awk for data processing

wget for file retrieval

Standard Unix tools (tail, mkdir, chmod, echo)

## ğŸ§  Author
Abdullah Mohammed â€“ Data Engineer
www.linkedin.com/in/abdullahgab
